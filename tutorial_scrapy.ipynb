{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar um projeto novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject projectname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar o shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell 'url'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este comando cria os seguintes diretórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # project items definition file\n",
    "\n",
    "        middlewares.py    # project middlewares file\n",
    "\n",
    "        pipelines.py      # project pipelines file\n",
    "\n",
    "        settings.py       # project settings file\n",
    "\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando spiders! O arquivo deve ser gravado na pasta <i>/projectname/projectname/spiders/spidername.py</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável name deve levar um nome único. De modo que possam ser criadas várias spiders no mesmo projeto, e que elas possam ser chamadas do terminal de comando executando :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl spidername"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na função start_request(self) será colocado as urls de onde as spiders irão extrair as informações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na função parse será definido de que parte daquela página será extraído os dados. Para isso é preciso abrir a página e estudar seu código fonte (HTML). Caso queiramos extrair todas as imagens da página, precisamos olhar o em que parte do código estão localizadas as imagens. No exemplo que aparecerá mais a frente, serão extraídos trechos de frases escritas por diversos autores. Esses textos estão entre tags <b>div</b>, <b>class quote</b>. \n",
    "\n",
    "No exemplo acima, QuotesSpiders, o código diz para salvar as informações extraídas em dois arquivos html,cada um com um nome diferente que será gerado de acordo com o número da página."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada \"quote\" estará em uma estrutura html como a seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('div.quote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = response.css('div.quote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = quote.css(\"span.text::text\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = quote.css(\"small.author::text\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = quote.css(\"div.tags a.tag::text\").getall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in response.css(\"div.quote\"):\n",
    "    text = quote.css(\"span.text::text\").get()\n",
    "    author = quote.css(\"small.author::text\").get()\n",
    "    tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "    print(dict(text=text, author=author, tags=tags))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais um exemplo de spider. Importante lembrar que cada spider deve ter seu nome único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note que a função parse() devolve um dicionário!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para registrar os dados dentro de um arquivo é necessário executar o scrapy com o seguinte comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl quotes -o quotes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vou agora testar o exemplo acima no site do portal da cidade de guaxupé. Quero extrair as manchetes da página de notícia.E quero testar de 15 em 15 dias para estudar o funcionamento do spider, quando houver mudanças no site, como atualização de notícias! Ele pegará as notícias novas ? ou precisaria reestruturar o código. Quando os sites realizam atualizações, como a inserção de novas notícias, eles mantém a estrutura em html? ou o código muda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".js-results  #CSS selector Esse é onde estão todas as manchetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<body>\n",
    "    <main> \n",
    "        <section class='container'>\n",
    "            <section class = 'main-content--flex'>\n",
    "                <article class='list-content'>\n",
    "                    <div class='js-results'>\n",
    "                        <a class='list-content--item' href='linkdecadamanchete'> </a>\n",
    "                            <div class='list-content--description'>\n",
    "                                #O título da notícia \n",
    "                                <h2 class='list-content--title grey-text text-darken-4'> </h2>\n",
    "                                #O subtítulo da notícia\n",
    "                                <p class='news-item--subtitle text-sm grey-text text-darken-2'> </p>\n",
    "                                #A data da notícia\n",
    "                                <p class='news-item--post-date text-xs grey-text text-darken-1'> </p>\n",
    "                                \n",
    "                        <a class='list-content--item' href='linkdecadamanchete'> </a>\n",
    "                        <a class='list-content--item' href='linkdecadamanchete'> </a>\n",
    "                        <a class='list-content--item' href='linkdecadamanchete'> </a>\n",
    "                    </div>\n",
    "                </article>\n",
    "            </section>\n",
    "        </section>\n",
    "    </main>\n",
    "</body>\n",
    "\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é o código html onde estão as primeiras manchetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = response.css('a.list-content--item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima irá gerar uma lista imensa de selectors,cada um correspondendo a uma manchete diferente. A idéia aqui seria extrair o título, o subtítulo,a data e o link da notícia, e criar inicialmente um dicionário.\n",
    "Para fazer isso, basta eu iterar por elas e extrair cada dado.Depois posso usar o dicionário para criar uma tabela com o panda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(noticias)):\n",
    "    yield {\n",
    "            'título': noticias[n].css('h2.list-content--title::text').get(),\n",
    "            'subtítulo': noticias[n].css('p.news-item--subtitle text-sm grey-text text-darken-2::text').get(),\n",
    "            'data': noticias[n].css('p.news-item--post-date text-xs grey-text text-darken-1::text').get(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = noticias[0].css('h2.list-content--title::text').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho html do título estará escrito assim,dentro da class <b>list-content--title grey-text text-darken-4</b>. Mas só preciso pegar 'h2.list.content--title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitulo = noticias[0].css('p.news-item--subtitle::text').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = noticias[0].css('p.news-item--post-date text-xs grey-text text-darken-1::text').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar uma spider, existe a possibilidade de executar o comando a seguir, que criará um modelo simples de spider, a ser preenchido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy genspider basic web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiz o teste com os códigos acima. Criei um projeto chamado guaxupe, os itens, o spider e coloquei as informações extraídas em um arquivo .json. deu certo!\n",
    "\n",
    "Mas preciso limpar as informações. colocar em algum lugar do código, a função .encode('utf-8') pra tratar as strings. Também tenho a intenção de extrair os links e criar uma tabela com o panda, só pra treinar.\n",
    "\n",
    "E depois preciso testar o spider a cada vez que inserirem mais notícias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo com o exemplo do spider Quote, proposto pelo tutorial do site oficial do scrapy.\n",
    "\n",
    "Caso eu queira pegar informações de todo o site, ao invés de extrair as informações de apenas uma página. Eu precisaria então acessar os links. Precisamos então olhar o código html da página e descobrir onde se encontram os links. Segue abaixo um link da página do exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<ul class=\"pager\">\n",
    "    <li class=\"next\">\n",
    "        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extrair esse link devemos colocar esse <i><b>::attr(href)</b></i> ao caminho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1] response.css('li.next a::attr(href)').get()\n",
    "In[1] '/page/2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou então utilizar a propriedade atributo disponível. <b><i>.attrib('href')</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1]    response.css('li.next a').attrib['href']\n",
    "Out [1]  '/page/2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo,segue modificações no spider, para que ele possa acessar links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de extrair as informações da página onde ele se encontra, o spider vai atrás do link para a próxima página e utilizando o método urljoin() e solicita um <b>request</b>, se regitrando como callback, ligado ao método parse para extrair as informações da próxima página.E continuar extraindo informações até que next_page seja None.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acessar links, é preciso criar regras de acordo com o site em que estamos, e também referente às informações que queremos extrair. No caso acima, criamos um script que acessa a página seguinte, e vai procurando por novos links, até que não haja mais 'próxima página'. Ele seria ideal para extrair informações de blogs, foruns e outros sites com paginação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferente de <i>scrapy.Request</i>, <i> response.follow</i> suporta urls relativos diretamente. Não precisa chamar urljoin . Note qie response.follow só retorna uma instância Request, você ainda precisa yield a Request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for href in response.css('ul.pager a::attr(href)'):\n",
    "    yield response.follow(href, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso utilizar o módulo json para trabalhar com as informações contidas em um arquivo .json. Anteriormente, havíamos extraído as manchetes de um site de notícia e havíamos armazenado em um dicionário e colocado tudo dentro de um arquivo .json. O próximo passo seria trabalhar com esses dados. Utilizá-los. Para isso podemos fazer o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jason\n",
    "json_file = open(\"/home/file.json\",\"r\",encoding = 'utf-8')\n",
    "file = json.load(json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se fizermos uma chamada file, no console do python, depois de ter executado os códigos acima, aparecerá na tela um dicionário,contendo todas as informações que extraímos.\n",
    "\n",
    "    Podemos depois utilizar o módulo pandas, para fazer um data frame com as informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pd.DataFrame(data=file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se executarmos df, no console python, após os códigos acima, teremos um dataframe com as informações coletadas.Mas como foram muitos dados coletados, e os subtítulos da manchetes tem um tamanho considerável, não vai caber tudo na tela. Um próximo ponto para eu investigar seria como transformar esse dataframe em uma planilha do excel, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fico imaginando como poderia utilizar essa ferramenta, pensando em uma empresa. Uma possibilidade, que foi apresentada no vídeo do coursera, é fazer uma pesquisa de preços com os concorrentes da empresa, para poder estabelecer o menor preço do mercado. \n",
    "\n",
    "Ou então poderia fazer um aplicativo que receberia informações, como por exemplo as manchetes de um jornal. O aplicativo seria atualizado automaticamente, a medida que o spider fosse executado e extraísse novo conteúdo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projeto spider 2 - dicionário com CNAEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://blog.meifacil.com/quero-ser-mei/atividades-mei-2020-quais-atividades-podem-ter-cnpj-mei/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = response.css('table.table')\n",
    "b = a.css('tbody')\n",
    "c = b.css('tr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
